# -*- coding: utf-8 -*-
"""notebook62ed20f733

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebook62ed20f733-789fe46c-36c0-4389-afeb-44aa35c68cca.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240402/auto/storage/goog4_request%26X-Goog-Date%3D20240402T205333Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D152404a77c7a390155a8dd82a743d81318c851cc9272b2fa9e1e5426f28cffa2949cd29054999d39ac9602cc966bf6f5521a0cd47851b39eac6652886211c58472c599f33be9f7fe9d8aad5291f614c0c31095a04fcee7192f5f3f36b321701b356a457a5e72f8a2927c0182570b167073432ed1040748583ba2bea9084581d8d51bbf58bed56ddfb5329fd1e50ffe8eb841ee7e0fa9e1f080a9d3b96be115b81bc0d6a61458531553639863b97e47f22c2347870354e27bf3580d6dfb5cba28257f20d8e6821764751cd94a2c92fd8fce29c65e385f03909afaba0de2bc028821423d39ced9010139441240002bb183c68ad23825d1b8640ce6013f1d277f7f
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'sign-language-mnist:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3258%2F5337%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240402%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240402T205332Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6a065a4f6667da8e48abff5179ec27c850ffdabcd2875f33b8e60ac3e9d0dc081946a96bda0a53884a3178e2307e6745677a614cbfd66b982ebcbb918730414b58b3eb5bce285980ef300cc8d0566b5cad1ae4f1cf3565682d8baf3185429acc96c98db42f1389dce0aa6e6ee74395f87d434757e4c6d73c72b11d8681eda02c8c578f0bd136faf2c6828fbdb2653a38d4ae21e16841c6110f1a4ce3f872b54bf4083b49d94239bb79f766cb108b1af7ce62d5ba49b4cf3e515289418457dc3ca651091d9ec169464684919c987a4c43aaaf62739accf6340070864be3d8cf17a503d34f9c8eb3626dd1511a1d6f7b1d6f5c818aa06644bbcabcba26a089543f'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""<iframe src="https://giphy.com/embed/HgycnYQCMeJXO" width="480" height="372" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/sky-hello-clouds-HgycnYQCMeJXO">via GIPHY</a></p>

<h1 style="font-family:verdana;"> <center><b>‚úã Sign Language MNIST Using CNNs</b></center> </h1>

# üëãWelcome to this notebook!
<div class="alert alert-block alert-info" style="font-size:20px; font-family:verdana;">
    Today, I would like to share a piece of work! A mini project which I have been working on..
    In this notebook I have trained a CNN on the Sign Language Dataset on Kaggle! I was able to achieve fairly reasonable results on the training and the validation data!
</div>

# So,before moving on what are CNNs?
<center><img src='https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9yF-a8gUktDKgRpuNuxNA.png'
     height=150px width=800px /></center>
     
<div style="background-color:#fff1cc; padding: 20px;">
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">Convolutional Neural Networks or CNNs stand as a cornerstone in modern machine learning and computer vision. Inspired by the human visual system, CNNs are a specialized class of artificial neural networks designed to excel at tasks like image recognition, object detection, and image classification. What sets CNNs apart is their ability to automatically learn and extract hierarchical features from raw pixel data, effectively capturing intricate patterns and structures within images. This is achieved through convolutional layers that apply filters to convolve across the input data, followed by pooling layers that reduce spatial dimensions while retaining important information. The resulting learned features enable CNNs to identify complex visual patterns with remarkable accuracy, making them invaluable tools in a wide array of applications, including medical imaging, self-driving cars, and facial recognition systems.</p>

<p style="font-size:20px; font-family:verdana; line-height: 1.7em">Convolutional Neural Networks (CNNs) have found extensive applications in various domains due to their exceptional ability to extract meaningful features from visual data. Here are eight prominent applications of CNNs in the modern world:</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Image Classification: CNNs excel at image classification tasks by learning and recognizing patterns within images, enabling them to accurately categorize objects, animals, and scenes, among other things.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Object Detection: CNNs are crucial for object detection in images and videos, enabling systems to identify and locate multiple objects within a scene, forming the backbone of technologies like autonomous vehicles and surveillance systems.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Face Recognition: CNNs power facial recognition systems, allowing them to identify and authenticate individuals from images or video frames, with applications in security, access control, and personalized user experiences.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Medical Imaging: In the medical field, CNNs aid in diagnosing diseases by analyzing medical images like X-rays, MRIs, and CT scans, assisting doctors in accurate and swift diagnosis.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Artificial Intelligence in Gaming: CNNs enhance gaming experiences by enabling real-time object tracking, gesture recognition, and character animation based on human movement and expressions.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Image Style Transfer: CNNs can transform the style of an image, transferring the artistic characteristics of one image to another, leading to novel visual effects and artistic creations.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Autonomous Vehicles: CNNs play a pivotal role in self-driving cars by detecting pedestrians, road signs, lanes, and obstacles, allowing vehicles to navigate safely and make informed decisions.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
--> Natural Language Processing (NLP): CNNs are also applied to NLP tasks, such as text classification and sentiment analysis, by converting textual data into image-like representations, enabling efficient feature extraction.</p>
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">
These applications showcase the versatility of CNNs and their impact on diverse fields, revolutionizing industries and enhancing our technological capabilities.</p>

# üèóÔ∏èImport Necessary Libraries

<p style="font-size:20px; font-family:verdana; line-height: 1.7em">We will now be importing several necessary libraries for the execution of this notebook in the next cell!.</p>
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img
import matplotlib.pyplot as plt
import numpy as np
import csv
import string
import graphviz
import pydot

"""# üìÑ Loading the data"""

validation = '/kaggle/input/sign-language-mnist/sign_mnist_test/sign_mnist_test.csv'

training = '/kaggle/input/sign-language-mnist/sign_mnist_train/sign_mnist_train.csv'

"""# ùëì Function to parse the data
<div style="background-color:#d4f1f4; padding: 20px;">
<p style="font-size:20px; font-family:verdana; line-height: 1.7em">We will now create a function called <b>parse_data(filename)</b> for helping us load the data, it will separate the images and labels from the file and as pretty evident it takes the filename as input <i>(also the filename here is the pathname as well)</i></p>
</div>
"""

# Parsing the dataset
def parse_data(filename):
    with open(filename) as file:
        csv_reader = csv.reader(file, delimiter=',')
        labels = []
        images = []
        next(csv_reader, None)
        for row in csv_reader:
            label = row[0]
            image = row[1:]
            image = np.array(image).reshape((28, 28))

            labels.append(label)
            images.append(image)

        images = np.array(images).astype(float)
        labels = np.array(labels).astype(float)
    return images, labels

"""## **Splitting the data into train and validation sets :**"""

training_images, training_labels = parse_data(training)
validation_images, validation_labels = parse_data(validation)

# Visualizing the shapes of the data
print(f"training images have shape : {training_images.shape}")
print(f"training labels have shape : {training_labels.shape}")
print(f"validation images have shape : {validation_images.shape}")
print(f"validation labels have shape : {validation_labels.shape}")

"""## **Expanding the dimensions of the images**
<div style="background-color:#fff1cc; padding: 20px;">

<p style="font-size:20px; font-family:verdana; line-height: 1.7em"><b>Why expand the dimensions you ask?</b><br>
    Well, because :<br>
    --> CNNs typically require input data to have a certain shape, often represented as (batch_size, height, width, channels). The "channels" dimension represents the color channels of the image, such as RGB (3 channels) or grayscale (1 channel). Expanding dimensions ensures that the input data complies with the required shape.
    </p>
</div>
"""

# Expanding the dimensions of the training and validation set to make them (10000, 28, 28, 1)
training_images = np.expand_dims(training_images, axis=3)
validation_images = np.expand_dims(validation_images, axis=3)

"""# ùëì Function to yield data
<div style="background-color:#d4f1f4; padding: 20px;">
<p style="font-size:18px; font-family:verdana; line-height: 1.7em">Now we will call <b>ImageDataGenerator</b> this will yield the batches of images for both the training and validation.</p>
<p style="font-size:18px; font-family:verdana; line-height: 1.7em">We will call the <b>ImageDataGenerator</b> in the function img_generator() which is defined in the cell below.</p>
<p style="font-size:18px; font-family:verdana; line-height: 1.7em"><b>The img_generator()</b> function will take the training and validation images as well as the labels for input.</p>
</div>
"""

# Creating an ImageDataGenerator for Image Augmentation
def img_generator(training_images, training_labels, validation_images, validation_labels):

    train_datagen = ImageDataGenerator(
                        rescale = 1./255,
                        rotation_range=40,
                        width_shift_range=0.2,
                        height_shift_range=0.2,
                        shear_range=0.2,
                        zoom_range=0.2,
                        horizontal_flip=True,
                        fill_mode='nearest')

    train_generator = train_datagen.flow(x=training_images, y=training_labels, batch_size=32)

    valid_datagen = ImageDataGenerator(rescale=1./255)

    valid_generator = valid_datagen.flow(x=validation_images, y=validation_labels, batch_size=32)

    return train_generator, valid_generator

# Testing the generator
train_generator, validation_generator = img_generator(training_images, training_labels,
                                                      validation_images, validation_labels)

print(f"Training generator images have shape : {train_generator.x.shape}")
print(f"Training generator labels have shape : {train_generator.y.shape}")
print(f"Validation generator images have shape : {validation_generator.x.shape}")
print(f"Validation generator labels have shape : {validation_generator.y.shape}")

# Creating the model
def model():

    model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=((28, 28, 1))),
        tf.keras.layers.MaxPool2D(2,2),
        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
        tf.keras.layers.MaxPool2D(2,2),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(512, activation='relu'),
        tf.keras.layers.Dense(26, activation='softmax')
    ])

    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    return model

model = model()

"""# ü§πTraining the model
<div style="background-color:#fff1cc; padding: 20px;">
  <p style="font-size:20px; font-family:verdana; line-height: 1.7em">In this step we will be training our model on the data and see what results it will bear! Also below are the model hyperparameters : </p>


"""

history = model.fit(train_generator, epochs = 20, validation_data = validation_generator)

model.summary()

"""# üìäPlotting the Classification Reports"""

# Plot the chart for accuracy and loss on both training and validation
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""# üìä Taking a look at the model"""

tf.keras.utils.plot_model(model, 'model.png', show_shapes=False, rankdir='LR')

"""***

<div style="color:white;
           display:fill;
           border-radius:5px;
           background-color:#5642C5;
           font-size:110%;
           font-family:Verdana;
           letter-spacing:0.5px">
        <p style="padding: 10px;
              color:white;">
            Thanks for viewing my work. If you like it, consider sharing it to others or give feedback to improve the notebook.
        </p>
    </div>

"""